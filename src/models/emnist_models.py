# AUTOGENERATED! DO NOT EDIT! File to edit: 06b_emnist_models.ipynb (unless otherwise specified).

__all__ = ['BayesianEMNISTCNN', 'EMnistOptimizerFactory', 'EMnistModelTrainer']

# Cell
from dataclasses import dataclass
from typing import Optional

import torch
import torch.nn
import torch.optim
from torch import nn as nn
from torch.nn import functional as F, Module
from torch.utils.data import DataLoader, Dataset

from active_learning import RandomFixedLengthSampler
from black_box_model_training import train
from consistent_mc_dropout import (
    BayesianModule,
    ConsistentMCDropout,
    ConsistentMCDropout2d,
    freeze_encoder_context
)

from model_optimizer_factory import ModelOptimizer, ModelOptimizerFactory

# Cell
from trained_model import ModelTrainer, TrainedModel, TrainedBayesianModel

class BayesianEMNISTCNN(BayesianModule):
    def __init__(self, num_classes=47):
        super().__init__()

        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv1_drop = ConsistentMCDropout2d()
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)
        self.conv2_drop = ConsistentMCDropout2d()
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)
        self.conv3_drop = ConsistentMCDropout2d()
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc1_drop = ConsistentMCDropout()
        self.fc2 = nn.Linear(512, num_classes)

    def mc_forward_impl(self, input: torch.Tensor, freeze_encoder: bool):
        with freeze_encoder_context(freeze_encoder):
            input = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(input)), 2))
            input = F.relu(self.conv2_drop(self.conv2(input)))
            input = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(input)), 2))
            input = input.view(-1, 128 * 4 * 4)
            input = F.relu(self.fc1_drop(self.fc1(input)))

        embedding = input
        input = self.fc2(input)
        input = F.log_softmax(input, dim=1)
        return input, embedding

# Cell


class EMnistOptimizerFactory(ModelOptimizerFactory):
    def create_model_optimizer(self) -> ModelOptimizer:
        model = BayesianEMNISTCNN()
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)
        return ModelOptimizer(model=model, optimizer=optimizer)


_dataloader_kwargs = dict(num_workers=4, pin_memory=True)


@dataclass
class EMnistModelTrainer(ModelTrainer):
    device: str

    num_training_samples: int = 1
    num_validation_samples: int = 20
    num_patience_epochs: int = 20
    max_training_epochs: int = 120

    min_samples_per_epoch: int = 1024
    num_training_batch_size: int = 64
    num_evaluation_batch_size: int = 128

    @staticmethod
    def create_model_optimizer() -> ModelOptimizer:
        model = BayesianEMNISTCNN()
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)
        return ModelOptimizer(model=model, optimizer=optimizer)

    def get_train_dataloader(self, dataset: Dataset):
        train_loader = DataLoader(
            dataset,
            batch_size=self.num_training_batch_size,
            sampler=RandomFixedLengthSampler(dataset, self.min_samples_per_epoch),
            drop_last=True,
            **_dataloader_kwargs
        )
        return train_loader

    def get_evaluation_dataloader(self, dataset: Dataset):
        evaluation_loader = DataLoader(
            dataset, batch_size=self.num_evaluation_batch_size, drop_last=False, shuffle=False, **_dataloader_kwargs
        )
        return evaluation_loader

    def get_trained(self, *, train_loader: DataLoader, train_augmentations: Optional[Module],
                    validation_loader: DataLoader, log, wandb_key_path:str, loss=None, validation_loss=None) -> TrainedModel:
        model_optimizer = self.create_model_optimizer()

        if loss is None:
            loss = torch.nn.NLLLoss()
        if validation_loss is None:
            validation_loss = torch.nn.NLLLoss()

        train(
            model=model_optimizer.model,
            optimizer=model_optimizer.optimizer,
            training_samples=self.num_training_samples,
            validation_samples=self.num_validation_samples,
            train_loader=train_loader,
            train_augmentations=train_augmentations,
            validation_loader=validation_loader,
            patience=self.num_patience_epochs,
            max_epochs=self.max_training_epochs,
            loss=loss,
            validation_loss=validation_loss,
            device=self.device,
            training_log=log,
            wandb_key_path=wandb_key_path,
        )

        return TrainedBayesianModel(model_optimizer.model)