# AUTOGENERATED! DO NOT EDIT! File to edit: 07_black_box_training.ipynb (unless otherwise specified).

__all__ = ['wandb_log_training', 'train', 'LOG_INTERVAL', 'train_with_schedule', 'train_with_cosine_annealing',
           'evaluate', 'evaluate_old', 'create_metrics']

# Internal Cell

from dataclasses import dataclass
from typing import Optional

import torch
import torch.utils.data
from blackhc.project import is_run_from_ipython
from blackhc.project.utils.ignite_progress_bar import ignite_progress_bar
from ignite.contrib.engines.common import setup_common_training_handlers
from ignite.contrib.handlers import ProgressBar
from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer
from ignite.metrics import Accuracy, Average, Loss, RunningAverage
from torch import nn

import wandb
from consistent_mc_dropout import (
    BayesianModule,
    GeometricMeanPrediction,
    LogProbMeanPrediction,
    SamplerModel,
    get_log_mean_probs,
    multi_sample_loss,
)
from experiment_logging import wandb_only
from restoring_early_stopping import (
    PatienceWithSnapshot,
    ReduceLROnPlateauWithScheduleWrapper,
    RestoringEarlyStopping,
    suggest_limit_schedule,
)
from trained_model import TrainedModel

# Cell


@wandb_only
def wandb_log_training(key_path, epochs_log, best_epoch, era_epochs):
    has_accuracy = all("accuracy" in metrics for metrics in epochs_log)
    has_crossentropy = all("crossentropy" in metrics for metrics in epochs_log)
    has_training_crossentropy = all("training_crossentropy" in metrics for metrics in epochs_log)

    val_data = []
    for i, metrics in enumerate(epochs_log):
        row = [i]
        if has_accuracy:
            row += [metrics["accuracy"]]
        if has_crossentropy:
            row += [metrics["crossentropy"]]
        if has_training_crossentropy:
            row += [metrics["training_crossentropy"]]
        val_data.append(row)

    val_fields = ["epoch"]

    if has_accuracy:
        val_fields += ["accuracy"]
    if has_crossentropy:
        val_fields += ["crossentropy"]
    if has_training_crossentropy:
        val_fields += ["training_crossentropy"]

    val_metrics_table = wandb.Table(
        data=val_data,
        columns=val_fields,
    )

    wandb_row = {}

    wandb_row[f"{key_path}/val_metrics"] = val_metrics_table

    # TODO: fix this because currently it does not look good/render :(
    # if has_accuracy:
    #     wandb_row[f"{key_path}/val_accuracy"] = wandb.plot.line(
    #         val_metrics_table, "epoch", "accuracy", title="Validation Accuracy"
    #     )
    # if has_crossentropy:
    #     wandb_row[f"{key_path}/val_crossentropy"] = wandb.plot.line(
    #         val_metrics_table, "epoch", "crossentropy", title="Validation Crossentropy"
    #     )
    # if has_training_crossentropy:
    #     wandb_row[f"{key_path}/val_training_crossentropy"] = wandb.plot.line(
    #         val_metrics_table, "epoch", "training_crossentropy", title="Validation Training Crossentropy"
    #     )

    if best_epoch is not None:
        wandb_row[f"{key_path}/best_epoch"] = best_epoch
        if has_accuracy:
            wandb_row[f"{key_path}/best_val_accuracy"] = epochs_log[best_epoch]["accuracy"]
        if has_crossentropy:
            wandb_row[f"{key_path}/best_val_crossentropy"] = epochs_log[best_epoch]["crossentropy"]

    if era_epochs is not None:
        wandb_row[f"{key_path}/era_epochs"] = era_epochs

    print(wandb_row)
    wandb.log(
        wandb_row,
        commit=False,
    )

# Cell

LOG_INTERVAL = 10


def train_simple(
    *,
    model,
    train_loader: torch.utils.data.DataLoader,
    validation_loader: torch.utils.data.DataLoader,
    patience: Optional[int],
    max_epochs: int,
    device: str,
    training_log: dict,
    wandb_key_path: str,
    loss=None,
    validation_loss=None,
    optimizer=None,
    prefer_accuracy=True,
    train_augmentations=None,
):
    if not len(train_loader.dataset):
        return optimizer

    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss

    train_model = model
    if train_augmentations is not None:
        train_model = torch.nn.Sequential(train_augmentations, train_model)

    validation_model = model

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    assert len(validation_loader.dataset) > 0, "Empty validation loader does not work with early stopping!!!"

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    # Add early stopping
    if patience is not None:
        if prefer_accuracy:

            def score_function():
                return float(validation_evaluator.state.metrics["accuracy"])

        else:

            def score_function():
                return float(-validation_evaluator.state.metrics["crossentropy"])

        early_stopping = RestoringEarlyStopping(
            patience=patience,
            score_function=score_function,
            module=model,
            optimizer=optimizer,
            training_engine=trainer,
            validation_engine=validation_evaluator,
        )
    else:
        early_stopping = None

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    best_epoch = None

    if early_stopping:
        training_log["best_epoch"] = early_stopping.best_epoch - 1
        best_epoch = early_stopping.best_epoch - 1

    # WandB support
    wandb_log_training(wandb_key_path, epochs_log, best_epoch=best_epoch, era_epochs=None)

    # Return the optimizer in case we want to continue training.
    return optimizer



def train(
    *,
    model,
    training_samples,
    validation_samples,
    train_loader: torch.utils.data.DataLoader,
    validation_loader: torch.utils.data.DataLoader,
    patience: Optional[int],
    max_epochs: int,
    device: str,
    training_log: dict,
    wandb_key_path: str,
    loss=None,
    validation_loss=None,
    optimizer=None,
    prefer_accuracy=True,
    train_augmentations=None,
):
    if not len(train_loader.dataset):
        return optimizer

    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss

    train_model = SamplerModel(model, training_samples)
    if train_augmentations is not None:
        train_model = torch.nn.Sequential(train_augmentations, train_model)

    validation_model = LogProbMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    assert len(validation_loader.dataset) > 0, "Empty validation loader does not work with early stopping!!!"

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    # Add early stopping
    if patience is not None:
        if prefer_accuracy:

            def score_function():
                return float(validation_evaluator.state.metrics["accuracy"])

        else:

            def score_function():
                return float(-validation_evaluator.state.metrics["crossentropy"])

        early_stopping = RestoringEarlyStopping(
            patience=patience,
            score_function=score_function,
            module=model,
            optimizer=optimizer,
            training_engine=trainer,
            validation_engine=validation_evaluator,
        )
    else:
        early_stopping = None

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    best_epoch = None

    if early_stopping:
        training_log["best_epoch"] = early_stopping.best_epoch - 1
        best_epoch = early_stopping.best_epoch - 1

    # WandB support
    wandb_log_training(wandb_key_path, epochs_log, best_epoch=best_epoch, era_epochs=None)

    # Return the optimizer in case we want to continue training.
    return optimizer

# Cell


def train_with_schedule(
    *,
    model,
    training_samples,
    validation_samples,
    train_loader: torch.utils.data.DataLoader,
    validation_loader: torch.utils.data.DataLoader,
    patience_schedule: [int],
    factor_schedule: [int],
    max_epochs: int,
    device: str,
    training_log: dict,
    wandb_key_path: str,
    loss=None,
    validation_loss=None,
    optimizer=None,
    prefer_accuracy=True,
    train_augmentations=None,
    limit_schedule: [int] = None,
):
    if not len(train_loader.dataset):
        return optimizer
    if not limit_schedule:
        limit_schedule, max_epochs = suggest_limit_schedule(patience_schedule, max_epochs)
        print(f"Limit schedule/max epochs updated: {limit_schedule}, {max_epochs}")

    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss

    train_model = SamplerModel(model, training_samples)
    if train_augmentations is not None:
        train_model = torch.nn.Sequential(train_augmentations, train_model)

    validation_model = LogProbMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    assert len(validation_loader.dataset) > 0, "Empty validation loader does not work with early stopping!!!"

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    if prefer_accuracy:

        def score_function(metrics):
            return float(metrics["accuracy"])

    else:

        def score_function(metrics):
            return float(metrics["crossentropy"])

    training_log["era_epochs"] = []

    def next_era_callback():
        training_log["era_epochs"].append(trainer.state.epoch)

    scheduler = ReduceLROnPlateauWithScheduleWrapper(
        optimizer,
        metrics_transform=score_function,
        factor_schedule=factor_schedule,
        patience_schedule=patience_schedule,
        limit_schedule=limit_schedule,
        end_callback=trainer.terminate,
        next_era_callback=next_era_callback,
        mode="max" if prefer_accuracy else "min",
        verbose=True,
        module=model,
    )

    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def step_scheduler(engine):
        scheduler.step(engine)

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    # WandB support
    wandb_log_training(wandb_key_path, epochs_log, best_epoch=None, era_epochs=training_log["era_epochs"])

    # Return the optimizer in case we want to continue training.
    return optimizer


def train_with_cosine_annealing(
    *,
    model,
    training_samples,
    validation_samples,
    train_loader: torch.utils.data.DataLoader,
    validation_loader: torch.utils.data.DataLoader,
    max_epochs: int,
    device: str,
    training_log: dict,
    wandb_key_path: str,
    loss=None,
    validation_loss=None,
    optimizer=None,
    train_augmentations=None,
):
    if not len(train_loader.dataset):
        return optimizer

    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss

    train_model = SamplerModel(model, training_samples)
    if train_augmentations is not None:
        train_model = torch.nn.Sequential(train_augmentations, train_model)

    validation_model = LogProbMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    # Only to look nicer.
    # RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")
    Average().attach(trainer, "training_crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        if len(validation_loader.dataset) > 0:
            ProgressBar(persist=False).attach(
                validation_evaluator,
                metric_names="all",
                event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
            )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    if len(validation_loader.dataset) > 0:

        @trainer.on(Events.EPOCH_COMPLETED)
        def compute_metrics(engine):
            validation_evaluator.run(validation_loader)

        @validation_evaluator.on(Events.EPOCH_COMPLETED)
        def log_validation_metrics(engine):
            metrics = dict(engine.state.metrics)
            epochs_log.append(metrics)

            if is_run_from_ipython():
                print(f"Epoch {trainer.state.epoch} metrics: {metrics}")

    else:

        # Logging
        @trainer.on(Events.EPOCH_COMPLETED)
        def log_training_metrics(engine):
            metrics = dict(engine.state.metrics)
            epochs_log.append(metrics)

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)

    @trainer.on(Events.EPOCH_COMPLETED)
    def step_scheduler(engine):
        scheduler.step()

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    # WandB support
    wandb_log_training(wandb_key_path, epochs_log, best_epoch=None, era_epochs=None)

    # Return the optimizer in case we want to continue training.
    return optimizer

# Cell


def evaluate(*, model: TrainedModel, loader, num_samples, device, storage_device, loss=None):
    log_probs_N_K_C, labels_N = model.get_log_probs_N_K_C_labels_N(
        loader=loader, num_samples=num_samples, device=device, storage_device=storage_device
    )

    if loss is None:
        loss = nn.NLLLoss()

    log_prob_mean_N_C = get_log_mean_probs(log_probs_N_K_C)
    crossentropy = loss(log_prob_mean_N_C, labels_N)
    accuracy = torch.sum(torch.eq(torch.argmax(log_prob_mean_N_C, dim=1), labels_N)).item() / len(labels_N)

    return dict(accuracy=accuracy, crossentropy=crossentropy)


def evaluate_old(*, model, num_samples, loader, device, loss=None):
                                # TODO: rewrite this on top of TrainedModel?
                                # Add "get_log_prob_predictions" which returns the mean?
                                # Compute accuracy etc based on that?

    # Move model to device
    model.to(device)

    evaluation_model = LogProbMeanPrediction(SamplerModel(model, num_samples))

    if loss is None:
        loss = nn.NLLLoss()

    metrics = create_metrics(loss)

    evaluator = create_supervised_evaluator(evaluation_model, metrics=metrics, device=device)

    ProgressBar(persist=False).attach(
        evaluator,
        metric_names="all",
        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
    )

    # Kick everything off
    evaluator.run(loader, max_epochs=1)

    return evaluator.state.metrics


def evaluate_simple(*, model, loader, device, loss=None):
    # TODO: rewrite this on top of TrainedModel?
    # Add "get_log_prob_predictions" which returns the mean?
    # Compute accuracy etc based on that?

    # Move model to device
    model.to(device)

    if loss is None:
        loss = nn.NLLLoss()

    metrics = create_metrics(loss)

    evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)

    ProgressBar(persist=False).attach(
        evaluator,
        metric_names="all",
        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
    )

    # Kick everything off
    evaluator.run(loader, max_epochs=1)

    return evaluator.state.metrics


def create_metrics(loss):
    return {"accuracy": Accuracy(), "crossentropy": Loss(loss)}