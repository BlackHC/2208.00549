# AUTOGENERATED! DO NOT EDIT! File to edit: 03_consistent_mc_dropout.ipynb (unless otherwise specified).

__all__ = ['GradEmbeddingType', 'PredictionsLabels', 'freeze_encoder_context', 'BayesianModule',
           'bmodule_get_embeddings', 'bmodule_get_grad_embeddings', 'bmodule_get_no_dropout_predictions_labels',
           'bmodule_get_predictions_labels', 'ConsistentMCDropout', 'ConsistentMCDropout2d', 'SamplerModel',
           'multi_sample_loss', 'geometric_mean_loss', 'GeometricMeanPrediction', 'LogProbMeanPrediction',
           'get_log_mean_probs', 'assert_no_shuffling_no_augmentations_dataloader',
           'get_bayesian_ensemble_predictions_labels']

# Cell
from dataclasses import dataclass
from enum import Enum
from functools import wraps
from typing import List
from contextlib import nullcontext

import numpy as np
import torch
from blackhc.progress_bar import create_progress_bar
from toma import toma
from torch.nn import Module

# Cell
from torch.utils import data


class GradEmbeddingType(Enum):
    BIAS = 0
    LINEAR = 1
    BIAS_LINEAR = 2

    def get_grad_embedding(self, embedding_N_K_E, loss_grad_N_K_C):
        embedding_dim = embedding_N_K_E.shape[2]
        num_classes = loss_grad_N_K_C.shape[2]

        if self != GradEmbeddingType.BIAS:
            # TODO: this seems very inefficient. Could do the same via broadcasting!
            # loss_grad_expanded_N_K_EC = torch.repeat_interleave(loss_grad_N_K_C, embedding_dim, dim=2)
            # grad_embedding_N_K_EC = loss_grad_expanded_N_K_EC * embedding_N_K_E.repeat(2, num_classes)
            loss_grad_N_K_1_C = loss_grad_N_K_C[:, :, None, :]
            embedding_N_K_E_1 = embedding_N_K_E[:, :, :, None]
            grad_embedding_N_K_EC = (loss_grad_N_K_1_C * embedding_N_K_E_1).flatten(2)

        if self == GradEmbeddingType.BIAS:
            return loss_grad_N_K_C
        elif self == GradEmbeddingType.LINEAR:
            return grad_embedding_N_K_EC
        elif self == GradEmbeddingType.BIAS_LINEAR:
            return torch.cat([loss_grad_N_K_C, grad_embedding_N_K_EC], dim=2)
        else:
            raise NotImplementedError(f"Unknown GradEmbeddingType {self}!")

    def get_grad_embedding_shape(self, N, K, embedding_B_L_E, loss_grad_B_L_C):
        embedding_dim = embedding_B_L_E.shape[2]
        num_classes = loss_grad_B_L_C.shape[2]

        if self == GradEmbeddingType.BIAS:
            grad_embedding_shape = (N, K, num_classes)
        elif self == GradEmbeddingType.LINEAR:
            grad_embedding_shape = (N, K, embedding_dim * num_classes)
        elif self == GradEmbeddingType.BIAS_LINEAR:
            grad_embedding_shape = (N, K, (embedding_dim + 1) * num_classes)
        else:
            raise NotImplementedError(f"Unknown GradEmbeddingType {self}!")

        return grad_embedding_shape


@dataclass
class PredictionsLabels:
    predictions: torch.Tensor
    labels: torch.Tensor


def freeze_encoder_context(freeze_encoder: bool):
    if freeze_encoder:
        return torch.no_grad()
    else:
        return nullcontext()


class BayesianModule(Module):
    """A module that we can sample multiple times from given a single input batch.

    To be efficient, the module allows for a part of the forward pass to be deterministic.

    If we sample with "0" samples, we do a single deterministic forward pass with disabled dropout during evaluation.
    """

    # num MC dropout samples
    num_samples = None

    def __init__(self):
        super().__init__()

    # Returns B x K x ...
    def forward(self, input_B: torch.Tensor, num_samples: int, return_embedding: bool=False, freeze_encoder: bool = False):
        BayesianModule.num_samples = num_samples

        if num_samples == 0:
            # No MC dropout (0 samples).
            features_B = self.deterministic_forward_impl(input_B, freeze_encoder=freeze_encoder)
            output_B, embedding_B = self.mc_forward_impl(features_B, freeze_encoder=freeze_encoder)
            output_B_1 = BayesianModule.unflatten_tensor(output_B, 1)
            embedding_B_1 = BayesianModule.unflatten_tensor(embedding_B, 1)
            return output_B_1, embedding_B_1

        features_B = self.deterministic_forward_impl(input_B, freeze_encoder=freeze_encoder)
        mc_features_BK = BayesianModule.mc_tensor(features_B, num_samples)
        mc_output_BK, mc_embedding_BK = self.mc_forward_impl(mc_features_BK, freeze_encoder=freeze_encoder)
        mc_output_B_K = BayesianModule.unflatten_tensor(mc_output_BK, num_samples)
        mc_embedding_B_K = BayesianModule.unflatten_tensor(mc_embedding_BK, num_samples)

        if return_embedding:
            return mc_output_B_K, mc_embedding_B_K
        else:
            return mc_output_B_K

    def deterministic_forward_impl(self, input_B: torch.Tensor, freeze_encoder:bool) -> torch.Tensor:
        return input_B

    def mc_forward_impl(self, mc_features_BK: torch.Tensor, freeze_encoder:bool) -> torch.Tensor:
        return mc_features_BK, mc_features_BK

    @staticmethod
    def unflatten_tensor(input: torch.Tensor, num_samples: int):
        input = input.view([-1, num_samples] + list(input.shape[1:]))
        return input

    @staticmethod
    def flatten_tensor(mc_input: torch.Tensor):
        return mc_input.flatten(0, 1)

    @staticmethod
    def mc_tensor(input: torch.tensor, num_samples: int):
        mc_shape = [input.shape[0], num_samples] + list(input.shape[1:])
        return input.unsqueeze(1).expand(mc_shape).flatten(0, 1)

    def get_no_dropout_predictions_labels(
        self, *, loader: data.DataLoader, device, storage_device,
    ):
        return bmodule_get_no_dropout_predictions_labels(
            self, loader=loader, device=device, storage_device=storage_device
        )

    def get_predictions_labels(
        self, *, num_samples: int, loader: data.DataLoader, device, storage_device
    ):
        return bmodule_get_predictions_labels(
            self,
            num_samples=num_samples,
            loader=loader,
            device=device,
            storage_device=storage_device,
        )

    def get_embeddings(
        self,
        *,
        num_samples: int,
        loader: data.DataLoader,
        device,
        storage_device,
    ):
        return bmodule_get_embeddings(
            self,
            num_samples=num_samples,
            loader=loader,
            device=device,
            storage_device=storage_device
        )

    def get_grad_embeddings(
        self,
        *,
        num_samples: int,
        loader: data.DataLoader,
        loss,
        grad_embedding_type: GradEmbeddingType,
        model_labels: bool,
        device,
        storage_device,
    ):
        return bmodule_get_grad_embeddings(
            self,
            num_samples=num_samples,
            loader=loader,
            loss=loss,
            grad_embedding_type=grad_embedding_type,
            model_labels=model_labels,
            device=device,
            storage_device=storage_device
        )


# Externalized method so we can easily auto-reload it.
@torch.no_grad()
def bmodule_get_embeddings(
    self: BayesianModule,
    *,
    num_samples: int,
    loader: data.DataLoader,
    device,
    storage_device,
):
    self.to(device=device)

    N = len(loader.dataset)
    embeddings = None

    real_num_samples = max(num_samples, 1)

    pbar = create_progress_bar(N * real_num_samples, tqdm_args=dict(desc="get_embeddings", leave=False))
    pbar.start()

    @toma.execute.range(0, real_num_samples, 128)
    def get_prediction_batch(start, end):
        nonlocal embeddings

        if start == 0:
            embeddings = None
            pbar.reset()

        self.eval()

        num_sub_samples = end - start

        data_start = 0
        for batch_x, batch_labels in loader:
            batch_x = batch_x.to(device=device, non_blocking=True)
            batch_predictions, batch_embeddings = self(
                batch_x, num_sub_samples if num_samples else 0, return_embedding=True, freeze_encoder=True
            )

            batch_size = len(batch_predictions)
            data_end = data_start + batch_size

            if embeddings is None:
                embeddings = torch.empty(
                    (N, real_num_samples, batch_embeddings.shape[2]), dtype=batch_embeddings.dtype, device=storage_device
                )

            embeddings[data_start:data_end, start:end].copy_(batch_embeddings, non_blocking=True)

            data_start = data_end

            pbar.update(batch_size * num_sub_samples)

    pbar.finish()

    return embeddings

def bmodule_get_grad_embeddings(
    self: BayesianModule,
    *,
    num_samples: int,
    loader: data.DataLoader,
    loss,
    grad_embedding_type: GradEmbeddingType,
    model_labels: bool,
    device,
    storage_device,
):
    self.to(device=device)

    N = len(loader.dataset)
    grad_embeddings = None

    wrapped_loss = multi_sample_loss(loss)

    real_num_samples = max(num_samples, 1)

    pbar = create_progress_bar(N * real_num_samples, tqdm_args=dict(desc="get_grad_embeddings", leave=False))
    pbar.start()

    @toma.execute.range(0, real_num_samples, 128)
    def get_prediction_batch(start, end):
        nonlocal grad_embeddings

        if start == 0:
            grad_embeddings = None
            pbar.reset()

        self.eval()

        num_sub_samples = end - start

        data_start = 0
        for batch_x, batch_labels in loader:
            batch_x = batch_x.to(device=device, non_blocking=True)
            batch_labels = batch_labels.to(device=device, non_blocking=True)
            batch_predictions, batch_embeddings = self(
                batch_x, num_sub_samples if num_samples else 0, return_embedding=True, freeze_encoder=True
            )
            batch_argmax = batch_predictions.argmax(dim=2)

            # Use the model's predicted label (batch_argmax) or the true labels (batch_labels)
            if model_labels:
                sum_loss = loss(batch_predictions.flatten(0,1), batch_argmax.flatten(0,1), reduction="sum")
            else:
                sum_loss = wrapped_loss(batch_predictions, batch_labels, reduction="sum")

            batch_loss_grad = torch.autograd.grad(sum_loss, batch_predictions)[0]

            batch_size = len(batch_predictions)
            data_end = data_start + batch_size

            if grad_embeddings is None:
                grad_embedding_shape = grad_embedding_type.get_grad_embedding_shape(
                    N, real_num_samples, batch_embeddings, batch_loss_grad
                )
                grad_embeddings = torch.empty(
                    grad_embedding_shape, dtype=batch_embeddings.dtype, device=storage_device
                )

            batch_grad_embedding = grad_embedding_type.get_grad_embedding(batch_embeddings, batch_loss_grad)
            grad_embeddings[data_start:data_end, start:end].copy_(batch_grad_embedding, non_blocking=True)

            data_start = data_end

            pbar.update(batch_size * num_sub_samples)

    pbar.finish()

    return grad_embeddings


@torch.no_grad()
def bmodule_get_no_dropout_predictions_labels(
    self: BayesianModule, *, loader: data.DataLoader, device, storage_device
):
    self.to(device=device)
    self.eval()

    N = len(loader.dataset)
    predictions = None
    labels = None

    pbar = create_progress_bar(N, tqdm_args=dict(desc="get_no_dropout_predictions_labels", leave=False))
    pbar.start()

    data_start = 0

    for batch_x, batch_labels in loader:
        batch_x = batch_x.to(device=device, non_blocking=True)
        batch_predictions = self(batch_x, num_samples=0)

        batch_size = len(batch_predictions)
        data_end = data_start + batch_size

        # Support multi-dim labels.
        if labels is None:
            labels_shape = (N, *batch_labels.shape[1:])
            labels = torch.empty(labels_shape, dtype=batch_labels.dtype, device=storage_device)
        # Support multi-dim predictions.
        if predictions is None:
            predictions_shape = (N, *batch_predictions.shape[1:])
            predictions = torch.empty(predictions_shape, dtype=batch_predictions.dtype, device=storage_device)

        predictions[data_start:data_end].copy_(batch_predictions, non_blocking=True)
        labels[data_start:data_end].copy_(batch_labels, non_blocking=True)

        data_start = data_end

        pbar.update(batch_size)

    pbar.finish()

    return predictions, labels


@torch.no_grad()
def bmodule_get_predictions_labels(
    self: BayesianModule, *, num_samples: int, loader: data.DataLoader, device, storage_device
):
    assert_no_shuffling_no_augmentations_dataloader(loader)

    if num_samples == 0:
        return self.get_no_dropout_predictions_labels(
            loader=loader, device=device, storage_device=storage_device
        )

    self.to(device=device)

    N = len(loader.dataset)
    predictions = None
    labels = None

    pbar = create_progress_bar(N * num_samples, tqdm_args=dict(desc="get_predictions_labels", leave=False))
    pbar.start()

    @toma.execute.range(0, num_samples, 128)
    def get_prediction_batch(start, end):
        nonlocal predictions
        nonlocal labels

        if start == 0:
            predictions = None
            labels = None
            pbar.reset()

        self.eval()

        num_sub_samples = end - start

        data_start = 0
        for batch_x, batch_labels in loader:
            batch_x = batch_x.to(device=device, non_blocking=True)
            batch_predictions = self(batch_x, num_sub_samples)

            batch_size = len(batch_predictions)
            data_end = data_start + batch_size

            # Support multi-dim predictions.
            if predictions is None:
                predictions_shape = (N, num_samples, *batch_predictions.shape[2:])
                predictions = torch.empty(predictions_shape, dtype=batch_predictions.dtype, device=storage_device)
            # Support multi-dim labels.
            if labels is None:
                labels_shape = (N, *batch_labels.shape[1:])
                labels = torch.empty(labels_shape, dtype=batch_labels.dtype, device=storage_device)

            predictions[data_start:data_end, start:end].copy_(batch_predictions, non_blocking=True)
            if start == 0:
                labels[data_start:data_end].copy_(batch_labels, non_blocking=True)

            data_start = data_end

            pbar.update(batch_size * num_sub_samples)

    pbar.finish()

    return predictions, labels

# Cell


class _ConsistentMCDropout(Module):
    def __init__(self, p=0.5):
        super().__init__()

        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))

        self.p = p
        self.mask = None

    def extra_repr(self):
        return "p={}".format(self.p)

    def reset_mask(self):
        self.mask = None

    def train(self, mode=True):
        super().train(mode)
        if not mode:
            self.reset_mask()

    def _get_sample_mask_shape(self, sample_shape):
        return sample_shape

    def _create_mask(self, input, num_samples):
        mask_shape = [1, num_samples] + list(self._get_sample_mask_shape(input.shape[1:]))
        mask = torch.empty(mask_shape, dtype=torch.bool, device=input.device).bernoulli_(self.p)
        return mask

    def forward(self, input: torch.Tensor):
        num_samples = BayesianModule.num_samples

        # Disable dropout during evaluation if num_samples is 0 (i.e. no samples).
        # Or if the dropout rate is 0.
        if self.p == 0.0 or num_samples == 0:
            return input

        if self.training:
            # Create a new mask on each call and for each batch element.
            num_samples = input.shape[0] * max(num_samples, 1)
            mask = self._create_mask(input, num_samples)
        else:
            if self.mask is None:
                # print('recreating mask', self)
                # Recreate mask.
                self.mask = self._create_mask(input, num_samples)

            mask = self.mask

        mc_input = BayesianModule.unflatten_tensor(input, num_samples)
        mc_output = mc_input.masked_fill(mask, 0) / (1 - self.p)

        # Flatten MCDI, batch into one dimension again.
        return BayesianModule.flatten_tensor(mc_output)

# Cell


class ConsistentMCDropout(_ConsistentMCDropout):
    r"""Randomly zeroes some of the elements of the input
    tensor with probability :attr:`p` using samples from a Bernoulli
    distribution. The elements to zero are randomized on every forward call during training time.

    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.

    This has proven to be an effective technique for regularization and
    preventing the co-adaptation of neurons as described in the paper
    `Improving neural networks by preventing co-adaptation of feature
    detectors`_ .

    Furthermore, the outputs are scaled by a factor of :math:`\frac{1}{1-p}` during
    training. This means that during evaluation the module simply computes an
    identity function.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: `Any`. Input can be of any shape
        - Output: `Same`. Output is of the same shape as input

    Examples::

        >>> m = nn.Dropout(p=0.2)
        >>> input = torch.randn(20, 16)
        >>> output = m(input)

    .. _Improving neural networks by preventing co-adaptation of feature
        detectors: https://arxiv.org/abs/1207.0580
    """
    pass


class ConsistentMCDropout2d(_ConsistentMCDropout):
    r"""Randomly zeroes whole channels of the input tensor.
    The channels to zero-out are randomized on every forward call.

    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.

    Usually the input comes from :class:`nn.Conv2d` modules.

    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.

    In this case, :func:`nn.Dropout2d` will help promote independence between
    feature maps and should be used instead.

    Args:
        p (float, optional): probability of an element to be zero-ed.
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Examples::

        >>> m = nn.Dropout2d(p=0.2)
        >>> input = torch.randn(20, 16, 32, 32)
        >>> output = m(input)

    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """

    def _get_sample_mask_shape(self, sample_shape):
        return [sample_shape[0]] + [1] * (len(sample_shape) - 1)

# Cell


class SamplerModel(Module):
    """Wrap a `BayesianModule` to sample k MC dropout samples consistently.

    A forward pass returns: log probs BxKxC for a batch of B inputs, K MC dropout samples, and C classes.
    """

    def __init__(self, bayesian_module: BayesianModule, num_samples: int):
        super().__init__()
        self.bayesian_module = bayesian_module
        self.num_samples = num_samples

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.bayesian_module(input, self.num_samples)
        return log_probs_B_K_C

# Cell


def multi_sample_loss(loss):
    """Wrap a loss function to expand the targets to work together with a SamplerModel."""

    @wraps(loss)
    def wrapped_loss(input_B_K_C, target_B_, *args, **kwargs):
        assert input_B_K_C.shape[0] == target_B_.shape[0]
        input_BK_C = input_B_K_C.flatten(0, 1)
        target_B_K_ = target_B_[:, None].expand(input_B_K_C.shape[:2] + target_B_.shape[1:])
        target_BK_ = target_B_K_.flatten(0, 1)
        return loss(input_BK_C, target_BK_, *args, **kwargs)

    return wrapped_loss

# Cell


def geometric_mean_loss(loss):
    @wraps(loss)
    def wrapped_loss(log_probs_B_K_C, target_N, *args, **kwargs):
        return loss(log_probs_B_K_C.mean(dim=1, keepdim=False), target_N, *args, **kwargs)

    return wrapped_loss


class GeometricMeanPrediction(Module):
    """
    Use a eg. SamplerModel and compute the geometric mean as "ensemble" prediction.

    We assume we receive log probs (so after Softmax).
    """

    def __init__(self, module: Module):
        super().__init__()
        self.module = module

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.module(input)
        return log_probs_B_K_C.mean(dim=1, keepdim=False)


class LogProbMeanPrediction(Module):
    """
    Use a eg. SamplerModel and compute the geometric mean as "ensemble" prediction.
    """

    def __init__(self, module: Module):
        super().__init__()
        self.module = module

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.module(input)
        log_mean_probs = log_probs_B_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_B_K_C.shape[1])
        return log_mean_probs


def get_log_mean_probs(log_probs_N_K_C):
    # Arithmetic mean of the probs (valid MC dropout)
    log_mean_probs_N_C = log_probs_N_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_N_K_C.shape[1])
    return log_mean_probs_N_C


def assert_no_shuffling_no_augmentations_dataloader(dataloader: data.DataLoader):
    batch_x_A = None
    batch_labels_A = None
    batch_x_B = None
    batch_labels_B = None

    for batch_x_A, batch_labels_A in dataloader:
        break

    for batch_x_B, batch_labels_B in dataloader:
        break

    assert torch.all(
        batch_x_A == batch_x_B
    ), "Batch inputs different. Augmentations enabled, or dataloader shuffles data?!"


    assert torch.all(batch_labels_A == batch_labels_B), "Batch labels different. Augmentations enabled, or dataloader shuffles data?!"


# TODO: remove this function?
def get_bayesian_ensemble_predictions_labels(*, modules: List[BayesianModule], k: int, loader: data.DataLoader, device):
    assert_no_shuffling_no_augmentations_dataloader(loader)

    ensemble_predictions = []
    ensemble_labels = None

    for module in modules:
        predictions, labels = module.get_predictions_labels(k=k, loader=loader, device=device)

        ensemble_predictions += [predictions]
        if ensemble_labels is not None:
            assert torch.all(ensemble_labels == labels)
        else:
            ensemble_labels = labels

        module.to("cpu")

    ensemble_predictions = torch.cat(ensemble_predictions, dim=1)
    return ensemble_predictions, ensemble_labels